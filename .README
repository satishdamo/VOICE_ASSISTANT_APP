VoiceChat Data Flow:

- User Interaction
  â†’ Button press triggers recording
- Audio Recording & Streaming
  â†’ MediaRecorder captures audio and sends chunks
- WebSocket Communication
  â†’ Sends audio chunks
  â† Receives transcript, LLM response, audio MIME, and audio chunks
- Backend Processing
  â†’ Transcription
  â†’ LLM response generation
  â†’ TTS synthesis
- Frontend Playback
  â†’ Uses MediaSource or falls back to Blob for audio playback
- UI Updates
  â†’ Displays transcript, response text, and playback progress

## ğŸ™ï¸ End of Stream: What It Means and Why It's Needed

### ğŸ“Œ What is "End of Stream"?

In our voice interaction pipeline, the **stream** refers to the continuous flow of audio chunks sent from the frontend to the backend over a WebSocket. These chunks are typically encoded (e.g. WebM) and sent in small intervals (e.g. every 250ms).

The **"end of stream"** is a signal that:

- The user has finished speaking
- The frontend has stopped recording
- No more audio chunks will be sent

This signal allows the backend to stop buffering and begin processing (e.g. transcription, LLM response, TTS synthesis).

---

### ğŸ› ï¸ Why the Backend Needs It

Without an explicit end-of-stream signal, the backend cannot reliably determine:

- Whether the user is done speaking
- Whether the user has paused briefly
- Whether the connection was interrupted

This could lead to:

- Hanging indefinitely while waiting for more chunks
- Premature processing based on unreliable timeouts

---

### âœ… How It Works

#### Frontend

After recording is complete, the frontend sends a special marker:

```ts
wsRef.current.send(encoder.encode("__END__"));
```
